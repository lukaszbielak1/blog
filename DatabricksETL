// Databricks notebook source

// COMMAND ----------

// DBTITLE 1,Odczytywanie pliku z zamontowanego Data Lake i prezentacja wynik√≥w
val df = spark.read.format("csv")
         .option("header", "true")
         .option("delimiter",";")
         .load("/mnt/tst1/adresy_swiadczen.csv")
display(df)



// COMMAND ----------

// DBTITLE 1,Transformacja danych 
import org.apache.spark.sql.functions.{concat, lit, when}
import org.apache.spark.sql.functions.regexp_replace

val df2 = 
df
.select("Nazwa zakladu", "Opis praktyki", "Miejscowosc", "Ulica", "Budynek", "Lokal", "Kod pocztowy")
.filter($"Nazwa zakladu" =!= "NULL")
.withColumn(
              "Adres", 
               concat(
                       $"Miejscowosc",
                       lit(", ul."),
                       regexp_replace($"Ulica" , "ul.",""),
                       lit(" "), 
                       $"Budynek", 
                       when($"Lokal" === "NULL", "").otherwise(concat(lit("/"),$"Lokal")) 
                     )
           )

display(df2)



// COMMAND ----------

// DBTITLE 1,Dodanie do bazy
df2
  .write
  .mode(SaveMode.Append) // <--- Append to the existing table
  .jdbc(jdbcUrl, "test", connectionProperties)

// COMMAND ----------

// DBTITLE 1,Odczyt z bazy
val df3 = spark.read.jdbc(jdbcUrl, "test", connectionProperties)
display(df3)

